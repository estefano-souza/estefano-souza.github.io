[
  {
    "objectID": "posts/fraud-model-kaggle/index.html",
    "href": "posts/fraud-model-kaggle/index.html",
    "title": "Modelos de Classificação de Fraude para Dados Desbalanceados",
    "section": "",
    "text": "A detecção de fraude é um processo, respectivamente, trabalhoso e desafiador para companhias de cartões de crédito por causa do volume enorme de transações completadas diariamente e porque muitas transações fraudulentas aparentam ser transações normais.\nUm modelo para identificação de transações fraudulentas é, tipicamente, um exemplo de classificação binária desbalanceada na qual o objetivo principal é classificar corretamente um evento positivo (no caso, uma transação fraudulenta). O termo “desbalanceado” vem do fato de que, em geral, o número de fraudes é muito menor que o número de transações normais, o que gera algumas dificuldades para que modelos tradicionais de classificação (como regressão logística ou árvore de decisão) realizem previsões precisas quanto ao evento positivo. Nessas condições, algumas métricas como a precisão e a rechamada (recall) são mais adequadas para avaliar a qualidade de um modelo preditivo de classificação binária se comparadas, por exemplo, à acurácia geral ou a área sob a curva ROC.\nNeste post, que foi inspirado no post original de Jason Brownlee no site Machine Learning Mastery (clique aqui para acessá-lo), nós utilizaremos o software R (R Core Team, 2025) para a importação e análise descritiva de dados de fraude via transações por cartão de crédito, assim como a construção e avaliação de dois modelos de classificação quando aplicados a dados desbalanceados em relação à distribuição de uma variável resposta que, por sua vez, identifica se uma transação foi fraudenta ou não. Vale observar que Brownlee fez toda a sua análise em Python e, aqui, o objetivo não é fazer uma comparação direta entre a utilização das duas linguagens (R e Python), mas trazer uma abordagem semelhante, ainda que seja por uma linguagem diferente.\nAntes de apresentarmos o conjunto de dados de fraude que será utilizado neste projeto, vamos carregar, dentro do ambiente R, os pacotes necessários para a nossa análise.\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(corrplot)\nlibrary(caret)\nlibrary(DescTools)\nlibrary(randomForest)\n\nAo longo deste projeto, a maioria dos procedimentos de preparação de dados será realizada a partir de funções do pacote dplyr (Wickham et al., 2023), que faz parte da coleção de pacotes tidyverse (Wickham, 2023). Para a construção das tabelas, serão utilizados os pacotes knitr (Xie, 2025), kableExtra (Zhu, 2024) e janitor (Firke, 2024). Os outros pacotes serão citados posteriormente."
  },
  {
    "objectID": "posts/fraud-model-kaggle/index.html#descrição-dos-dados",
    "href": "posts/fraud-model-kaggle/index.html#descrição-dos-dados",
    "title": "Modelos de Classificação de Fraude para Dados Desbalanceados",
    "section": "\n2.1 Descrição dos Dados",
    "text": "2.1 Descrição dos Dados\nRealizou-se a coleta dos dados ao longo de dois dias, de modo que foram detectadas 492 fraudes, o que representa apenas 0,172% de todas as 284807 transações realizadas no período.\nDevido a questões de confidencialidade, as 28 variáveis de entrada originais foram transformadas por meio de Análise de Componentes Principais (PCA), resultando nas componentes V1, V2, \\(\\cdots\\), V28 que serão nossas candidatas a variáveis preditoras.\nA variável time representa o tempo, em segundos, entre a primeira transação e a respectiva transação, de modo que as transações já estão ordenadas em ordem cronológica no conjunto de dados original e, portanto, a primeira linha representa a primeira transação realizada. Por sua vez, a variável amount contém o valor monetário (provavelmente em euros) de cada transação - por simplificação, não a utilizaremos como um preditor nos dois modelos de classificação de fraude.\nFinalmente, a variável class é a nossa variável resposta, assumindo o valor 1 em caso de fraude e o valor 0 caso contrário. Como dito anteriormente, somente 0,172% das transações foram detectadas como fraudulentas, o que invialibiliza o uso da estatística de acurácia como uma ferramenta totalmente confiável de avaliação da qualidade geral do ajuste do modelo. Outras estatísticas como precisão (precision) e rechamada (recall), além da própria área sobre a curva de precisão-rechamada (AUPRC), são mais adequadas quando os dados são muito desbalanceados em relação às categorias da variável resposta. Em particular, nós trabalharemos especialmente com a precisão e a rechamada."
  },
  {
    "objectID": "posts/fraud-model-kaggle/index.html#importação-dos-dados",
    "href": "posts/fraud-model-kaggle/index.html#importação-dos-dados",
    "title": "Modelos de Classificação de Fraude para Dados Desbalanceados",
    "section": "\n2.2 Importação dos Dados",
    "text": "2.2 Importação dos Dados\nPara facilitar a reprodutibilidade da análise, o conjunto de dados original foi previamente dividido em cinco arquivos do tipo CSV que estão armazenados no meu repositório pessoal. Cada arquivo contém os nomes das variáveis na primeira linha e pode ser importado como um data frame a partir da função read.csv, como exibido neste trecho de código:\n\ncreditcard_01 &lt;- read.csv(paste0(\"https://raw.githubusercontent.com/estefano-souza\",\n                         \"/projeto-final/refs/heads/main/dados/creditcard_01.csv\"),\n                          header=TRUE,\n                          stringsAsFactors=FALSE)\ncreditcard_02 &lt;- read.csv(paste0(\"https://raw.githubusercontent.com/estefano-souza\",\n                         \"/projeto-final/refs/heads/main/dados/creditcard_02.csv\"),\n                          header=TRUE,\n                          stringsAsFactors=FALSE)\ncreditcard_03 &lt;- read.csv(paste0(\"https://raw.githubusercontent.com/estefano-souza\",\n                         \"/projeto-final/refs/heads/main/dados/creditcard_03.csv\"),\n                          header=TRUE,\n                          stringsAsFactors=FALSE)\ncreditcard_04 &lt;- read.csv(paste0(\"https://raw.githubusercontent.com/estefano-souza\",\n                         \"/projeto-final/refs/heads/main/dados/creditcard_04.csv\"),\n                          header=TRUE,\n                          stringsAsFactors=FALSE)\ncreditcard_05 &lt;- read.csv(paste0(\"https://raw.githubusercontent.com/estefano-souza\",\n                         \"/projeto-final/refs/heads/main/dados/creditcard_05.csv\"),\n                          header=TRUE,\n                          stringsAsFactors=FALSE)"
  },
  {
    "objectID": "posts/fraud-model-kaggle/index.html#preparação-dos-dados",
    "href": "posts/fraud-model-kaggle/index.html#preparação-dos-dados",
    "title": "Modelos de Classificação de Fraude para Dados Desbalanceados",
    "section": "\n2.3 Preparação dos Dados",
    "text": "2.3 Preparação dos Dados\nA seguir, os cinco data frames são unidos, de forma sequencial, para formar um único data frame chamado creditcard que contém todas as transações realizadas. Contudo, a primeira variável (coluna sem nome) contém o índice original de cada transação no conjunto de dados: dentro do R, ela recebe o nome automático X e os dados são ordenados a partir dos seus valores para retomarmos a ordenação original.\nApós a união, a variável X é imediatamente excluída e, finalmente, os cinco data frames originais são excluídos do ambiente de trabalho.\n\ncreditcard &lt;- dplyr::bind_rows(creditcard_01, creditcard_02,\n                              creditcard_03, creditcard_04,\n                              creditcard_05) |&gt; \n              dplyr::arrange(X) |&gt; \n              dplyr::select(!X)\n\nremove(list = sprintf(\"creditcard_0%d\", seq(1:5)))\n\nA seguir, apresentamos uma visualização básica das primeiras transações (linhas) do conjunto completo de dados na Tabela 1. Observe que todas as variáveis são numéricas, incluindo a variável class.\n\n\n\nTabela 1: Conjunto de dados de fraude (primeiras transações)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\nV10\nV11\nV12\nV13\nV14\nV15\nV16\nV17\nV18\nV19\nV20\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\namount\nclass\n\n\n\n0\n-1.3598\n-0.0728\n2.5363\n1.3782\n-0.3383\n0.4624\n0.2396\n0.0987\n0.3638\n0.0908\n-0.5516\n-0.6178\n-0.9914\n-0.3112\n1.4682\n-0.4704\n0.2080\n0.0258\n0.4040\n0.2514\n-0.0183\n0.2778\n-0.1105\n0.0669\n0.1285\n-0.1891\n0.1336\n-0.0211\n149.62\n0\n\n\n0\n1.1919\n0.2662\n0.1665\n0.4482\n0.0600\n-0.0824\n-0.0788\n0.0851\n-0.2554\n-0.1670\n1.6127\n1.0652\n0.4891\n-0.1438\n0.6356\n0.4639\n-0.1148\n-0.1834\n-0.1458\n-0.0691\n-0.2258\n-0.6387\n0.1013\n-0.3398\n0.1672\n0.1259\n-0.0090\n0.0147\n2.69\n0\n\n\n1\n-1.3584\n-1.3402\n1.7732\n0.3798\n-0.5032\n1.8005\n0.7915\n0.2477\n-1.5147\n0.2076\n0.6245\n0.0661\n0.7173\n-0.1659\n2.3459\n-2.8901\n1.1100\n-0.1214\n-2.2619\n0.5250\n0.2480\n0.7717\n0.9094\n-0.6893\n-0.3276\n-0.1391\n-0.0554\n-0.0598\n378.66\n0\n\n\n1\n-0.9663\n-0.1852\n1.7930\n-0.8633\n-0.0103\n1.2472\n0.2376\n0.3774\n-1.3870\n-0.0550\n-0.2265\n0.1782\n0.5078\n-0.2879\n-0.6314\n-1.0596\n-0.6841\n1.9658\n-1.2326\n-0.2080\n-0.1083\n0.0053\n-0.1903\n-1.1756\n0.6474\n-0.2219\n0.0627\n0.0615\n123.50\n0\n\n\n2\n-1.1582\n0.8777\n1.5487\n0.4030\n-0.4072\n0.0959\n0.5929\n-0.2705\n0.8177\n0.7531\n-0.8228\n0.5382\n1.3459\n-1.1197\n0.1751\n-0.4514\n-0.2370\n-0.0382\n0.8035\n0.4085\n-0.0094\n0.7983\n-0.1375\n0.1413\n-0.2060\n0.5023\n0.2194\n0.2152\n69.99\n0\n\n\n2\n-0.4260\n0.9605\n1.1411\n-0.1683\n0.4210\n-0.0297\n0.4762\n0.2603\n-0.5687\n-0.3714\n1.3413\n0.3599\n-0.3581\n-0.1371\n0.5176\n0.4017\n-0.0581\n0.0687\n-0.0332\n0.0850\n-0.2083\n-0.5598\n-0.0264\n-0.3714\n-0.2328\n0.1059\n0.2538\n0.0811\n3.67\n0\n\n\n4\n1.2297\n0.1410\n0.0454\n1.2026\n0.1919\n0.2727\n-0.0052\n0.0812\n0.4650\n-0.0993\n-1.4169\n-0.1538\n-0.7511\n0.1674\n0.0501\n-0.4436\n0.0028\n-0.6120\n-0.0456\n-0.2196\n-0.1677\n-0.2707\n-0.1541\n-0.7801\n0.7501\n-0.2572\n0.0345\n0.0052\n4.99\n0\n\n\n7\n-0.6443\n1.4180\n1.0744\n-0.4922\n0.9489\n0.4281\n1.1206\n-3.8079\n0.6154\n1.2494\n-0.6195\n0.2915\n1.7580\n-1.3239\n0.6861\n-0.0761\n-1.2221\n-0.3582\n0.3245\n-0.1567\n1.9435\n-1.0155\n0.0575\n-0.6497\n-0.4153\n-0.0516\n-1.2069\n-1.0853\n40.80\n0\n\n\n7\n-0.8943\n0.2862\n-0.1132\n-0.2715\n2.6696\n3.7218\n0.3701\n0.8511\n-0.3920\n-0.4104\n-0.7051\n-0.1105\n-0.2863\n0.0744\n-0.3288\n-0.2101\n-0.4998\n0.1188\n0.5703\n0.0527\n-0.0734\n-0.2681\n-0.2042\n1.0116\n0.3732\n-0.3842\n0.0117\n0.1424\n93.20\n0\n\n\n9\n-0.3383\n1.1196\n1.0444\n-0.2222\n0.4994\n-0.2468\n0.6516\n0.0695\n-0.7367\n-0.3668\n1.0176\n0.8364\n1.0068\n-0.4435\n0.1502\n0.7395\n-0.5410\n0.4767\n0.4518\n0.2037\n-0.2469\n-0.6338\n-0.1208\n-0.3850\n-0.0697\n0.0942\n0.2462\n0.0831\n3.68\n0"
  },
  {
    "objectID": "posts/fraud-model-kaggle/index.html#estatísticas-descritivas",
    "href": "posts/fraud-model-kaggle/index.html#estatísticas-descritivas",
    "title": "Modelos de Classificação de Fraude para Dados Desbalanceados",
    "section": "\n3.1 Estatísticas Descritivas",
    "text": "3.1 Estatísticas Descritivas\nComo esperado, a distribuição da variável class se manteve semelhante à distribuição original tanto na partição de treinamento (Tabela 2) quanto na partição de teste (Tabela 3).\n\n\n\nTabela 2: Distribuição da variável resposta (partição de treinamento)\n\n\n\n\nclass\nFrequência\nPorcentagem\n\n\n\n0\n227453\n99.828\n\n\n1\n393\n0.172\n\n\nTotal\n227846\n100.000\n\n\n\n\n\n\n\n\n\n\n\nTabela 3: Distribuição da variável resposta (partição de teste)\n\n\n\n\nclass\nFrequência\nPorcentagem\n\n\n\n0\n56862\n99.826\n\n\n1\n99\n0.174\n\n\nTotal\n56961\n100.000\n\n\n\n\n\n\n\n\nA partição de treinamento train_data contém 227846 transações, o que representa um pouco mais de 80% de todas as transações. Por sua vez, a partição de teste test_data contém 56961 transações, um pouco menos de 20% do total.\nComo um exemplo de análise de cada variável candidata a preditor, apresentamos as estatísticas descritivas básicas da variável V1 na partição de treinamento (Tabela 4) e na partição de teste (Tabela 5), agrupadas pelos valores da variável resposta class em cada partição. Em particular, observe que, levando-se em conta somente as transações fraudulentas (class = 1), os valores dessas estatísticas são semelhantes ao compararmos as duas partições, mesmo com apenas 492 transações deste tipo no conjunto de dados completo.\n\n\n\nTabela 4: Estatísticas descritivas da variável V1 na partição de treinamento (agrupadas pela variável resposta)\n\n\n\n\nclass\nMédia\nMediana\nDesvio Padrão\nMínimo\nMáximo\n\n\n\n0\n0.004\n0.011\n0.986\n-28.852\n1.256\n\n\n1\n-2.392\n-1.231\n3.327\n-14.936\n1.091\n\n\n\n\n\n\n\n\n\n\n\nTabela 5: Estatísticas descritivas da variável V1 na partição de teste (agrupadas pela variável resposta)\n\n\n\n\nclass\nMédia\nMediana\nDesvio Padrão\nMínimo\nMáximo\n\n\n\n0\n0.005\n0.005\n0.981\n-23.745\n1.237\n\n\n1\n-2.609\n-1.104\n3.964\n-15.483\n1.061"
  },
  {
    "objectID": "posts/fraud-model-kaggle/index.html#matriz-de-correlações-dos-preditores",
    "href": "posts/fraud-model-kaggle/index.html#matriz-de-correlações-dos-preditores",
    "title": "Modelos de Classificação de Fraude para Dados Desbalanceados",
    "section": "\n3.2 Matriz de Correlações dos Preditores",
    "text": "3.2 Matriz de Correlações dos Preditores\nComo explicado na Seção 2, as 28 variáveis de entrada originais foram transformadas em componentes principais para manter a confidencialidade das informações sobre os clientes. Componentes construídas via PCA não são correlacionadas, o que favorece a sua utilização como preditores em modelos como a regressão logística, que supõe a ausência de colinearidade (correlação) perfeita entre as variáveis independentes.\nApenas como uma verificação simples, exibimos uma representação gráfica da matriz de correlações de Pearson das variáveis padronizadas V1 a V28 para os dados da partição de treinamento (Figura 1) e da partição de teste (Figura 2). O cálculo das correlações e a visualização dos valores é feita a partir da função corrplot do pacote [corrplot] (Wei & Simko, 2024).\nDe fato, os valores de todas as correlações entre cada par de variáveis distintas estão muito próximos de zero, independentemente da partição.\n\n\n\n\nFigura 1: Matriz de correlações dos preditores (amostra de treinamento)\n\n\n\n\n\n\n\n\n\n\n\nFigura 2: Matriz de correlações dos preditores (amostra de teste)\n\n\n\n\n\n\n\nNas próximas duas seções, vamos construir dois modelos de classificação e, então, comparar seus desempenhos em termos das estatísticas de qualidade do ajuste para as previsões de ocorrência de fraude nos dados da partição de teste."
  },
  {
    "objectID": "posts/fraud-model-kaggle/index.html#construção-do-modelo-logístico",
    "href": "posts/fraud-model-kaggle/index.html#construção-do-modelo-logístico",
    "title": "Modelos de Classificação de Fraude para Dados Desbalanceados",
    "section": "\n4.1 Construção do Modelo Logístico",
    "text": "4.1 Construção do Modelo Logístico\nPara a estimação dos parâmetros do nosso modelo de regressão logística, vamos usar a função glm com as opções padrão para a construção deste tipo de modelo (família binomial com função de ligação \\(logit(p)=\\log{(\\frac{p}{1 - p})}\\), onde \\(0 &lt; p &lt; 1\\) é a probabilidade de ocorrência do evento positivo) e utilizando os dados da partição de treinamento train_data.\n\nmodelo_trn &lt;- glm(class ~ ., family = binomial(link = \"logit\"),\n                  data = train_data)\n\nA seguir, vamos apresentar os resultados do modelo e avaliar a sua capacidade de fazer boas previsões a partir dos dados da partição de teste."
  },
  {
    "objectID": "posts/fraud-model-kaggle/index.html#resumo-do-modelo-logístico",
    "href": "posts/fraud-model-kaggle/index.html#resumo-do-modelo-logístico",
    "title": "Modelos de Classificação de Fraude para Dados Desbalanceados",
    "section": "\n4.2 Resumo do Modelo Logístico",
    "text": "4.2 Resumo do Modelo Logístico\n\n4.2.1 Coeficientes Estimados e Significância\nA função summary aplicada ao modelo construído gera o seguinte resumo com os valores das estimativas dos coeficientes de cada variável preditora e os respectivos testes de significância estatística.\n\nsummary(modelo_trn)\n\n\nCall:\nglm(formula = class ~ ., family = binomial(link = \"logit\"), data = train_data)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -8.721792   0.167508 -52.068  &lt; 2e-16 ***\nV1           0.189860   0.093878   2.022 0.043134 *  \nV2          -0.135743   0.096064  -1.413 0.157642    \nV3           0.040672   0.077375   0.526 0.599132    \nV4           1.006606   0.117597   8.560  &lt; 2e-16 ***\nV5           0.057593   0.097196   0.593 0.553486    \nV6          -0.044451   0.113455  -0.392 0.695211    \nV7          -0.070569   0.079068  -0.893 0.372117    \nV8          -0.240317   0.038403  -6.258 3.90e-10 ***\nV9          -0.263817   0.139162  -1.896 0.057992 .  \nV10         -0.951669   0.128120  -7.428 1.10e-13 ***\nV11         -0.028186   0.090013  -0.313 0.754184    \nV12         -0.089199   0.094425  -0.945 0.344834    \nV13         -0.286156   0.091324  -3.133 0.001728 ** \nV14         -0.510000   0.066357  -7.686 1.52e-14 ***\nV15         -0.074151   0.088211  -0.841 0.400567    \nV16         -0.167047   0.124229  -1.345 0.178730    \nV17         -0.005460   0.066263  -0.082 0.934329    \nV18          0.005955   0.122206   0.049 0.961136    \nV19          0.041842   0.089990   0.465 0.641955    \nV20         -0.263711   0.064021  -4.119 3.80e-05 ***\nV21          0.306188   0.049372   6.202 5.59e-10 ***\nV22          0.413381   0.110591   3.738 0.000186 ***\nV23         -0.094399   0.049805  -1.895 0.058043 .  \nV24          0.021225   0.103938   0.204 0.838192    \nV25         -0.014582   0.079452  -0.184 0.854383    \nV26          0.039049   0.105380   0.371 0.710972    \nV27         -0.295135   0.065240  -4.524 6.07e-06 ***\nV28         -0.090932   0.044651  -2.037 0.041699 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5786.3  on 227845  degrees of freedom\nResidual deviance: 1643.8  on 227817  degrees of freedom\nAIC: 1701.8\n\nNumber of Fisher Scoring iterations: 12\n\n\nAlém da estimativa do intercepto do preditor linear, as estivativas dos coeficientes das componentes V4, V8, V10, V14, V20, V21, V22 e V27 foram detectadas como estatisticamente significantes em um nível de significância de 0,1% - faz sentido utilizar um nível tão pequeno como referência devido ao tamanho amostral relativamente grande dos conjuntos de dados. No momento, vamos manter todos os preditores no modelo, mas será um exercício interessante comparar o desempenho do modelo atual com um modelo que contém somente os preditores significantes, o que será feito no final da seção.\n\n4.2.2 Pseudo R-Quadrado\nComo uma forma de avaliar a qualidade geral do ajuste a partir dos dados de treinamento, a Tabela 6 exibe os valores das medidas de pseudo R-quadrado de McFadden, Cox-Snell e Nagelkerke (Cragg-Uhler). Foi utilizada a função PseudoR2 do pacote [DescTools] (Signorell, 2025) para o cálculo destas medidas.\n\nDescTools::PseudoR2(modelo_trn,\n                    which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\")) |&gt;\n  knitr::kable(digits = 5,\n               col.names = c(\"Medida\", \"Valor\"))\n\n\nTabela 6: Medidas de pseudo R-quadrado para o modelo logístico\n\n\n\n\nMedida\nValor\n\n\n\nMcFadden\n0.71591\n\n\nCoxSnell\n0.01802\n\n\nNagelkerke\n0.71849\n\n\n\n\n\n\n\n\nEm geral, os valores destas medidas não são próximos de 1 mesmo em modelos de classificação relativamente bem ajustados. Particularmente, o limite superior do pseudo R-quadrado de Cox-Snell sempre é menor que 1; a medida de Nagelkerke, de fato, é uma versão “normalizada” da medida de Cox-Snell para que seja garantido que o limite superior sempre seja 1.\nTanto o pseudo R-quadrado de McFadden quanto o de Nagelkerke tem o valor 1 como limite superior, o que os aproxima de uma interpretação semelhante à do R-quadrado da regressão linear. Contudo, em situações nas quais os dados são muito desbalanceados em relação às categorias de uma variável resposta binária, os valores de ambas as medidas ficam próximas de 1 de forma artificial porque a categoria com a maior frequência tende a ser muito melhor identificada pelo modelo do que a categoria com a menor frequência, resultando numa contribuição igualmente desbalanceada na qualidade do ajuste (como veremos no próximo tópico).\nNo nosso modelo logístico, ambas as medidas estão próximas de 0,72 (ou 72%), o que poderia nos levar à conclusão de que o ajuste do modelo aos dados da partição de treinamento está satisfatório. Entretanto, nossa breve discussão no parágrafo anterior deixa evidente que precisaremos utilizar outras métricas para avaliar a qualidade do nosso modelo.\nPara mais detalhes sobre medidas de pseudo R-quadrado, você pode consultar o artigo que está disponível neste link: https://statisticalhorizons.com/r2logistic/.\n\n4.2.3 Avaliação das Previsões do Modelo Logístico\nPara verificarmos quão bem nosso modelo de regressão logística binária fez suas previsões, vamos avaliar a matriz de confusão e suas principais estatísticas associadas tanto para os dados da partição de treinamento quanto - e principalmente - para a amostra de teste.\nPartição de Treinamento (Modelo Logístico)\nVamos começar com a partição de treinamento. As previsões do modelo logístico (1 para “fraude” e 0 para “não-fraude”) são armazenadas em um vetor numérico chamado previsoes_logit_trn e, então, recuperamos os valores originais da variável resposta (train_data$class) para utilizarmos estes dois objetos como argumentos da função confusionMatrix do pacote [caret] que retorna um objeto com todas as informações que precisaremos. Para esta função, é necessário que os vetores numéricos com as previsões e os valores observados sejam transformados em fatores. Além disto, observe que definimos o indicador de fraude como evento positivo (positive = \"1\") para que as estatísticas de precisão e rechamada sejam calculadas em função deste evento.\n\nprevisoes_logit_trn &lt;- ifelse(predict(modelo_trn,\n                                  type = \"response\") &gt; 0.5, 1, 0)\n\ncm_trn &lt;- caret::confusionMatrix(data = as.factor(previsoes_logit_trn),\n                              reference = as.factor(train_data$class),\n                              dnn = c(\"Previsto\", \"Observado\"),\n                              positive = \"1\",\n                              mode = \"everything\")\n\nA seguir, a Figura 3 exibe tanto a matriz de confusão (também conhecida como matriz de classificação) quanto algumas das principais estatísticas de avaliação da qualidade do ajuste do modelo. Nós discutiremos cada um destes elementos a seguir.\n\ndraw_confusion_matrix(cm_trn)\n\n\n\n\n\nFigura 3: Matriz de confusão e estatísticas da qualidade das previsões do modelo logístico na partição de treinamento\n\n\n\n\n\n\n\n(OBSERVAÇÃO: O código original com a função draw_confusion_matrix que constroi esta visualização está disponível em https://stackoverflow.com/questions/23891140/r-how-to-visualize-confusion-matrix-using-the-caret-package. Seu único argumento é um objeto construído previamente pela função confusionMatrix. Nós fizemos algumas alterações no código da função para adaptá-la à nossa análise.)\nA acurácia é a porcentagem de transações, em relação ao total, que foram classificadas corretamente pelo modelo, independentemente da categoria da variável resposta class. Na nossa análise, este valor é 99,93%, o que indica a priori que o modelo fez um trabalho quase perfeito ao classificar as transações nos dados de treinamento, mas é necessário analisarmos as previsões para cada categoria de forma separada, especialmente quando os dados são tão desbalanceados.\nSomente 33 das 227453 transações não fraudulentas na partição de treinamento foram classificadas como fraude pelo modelo, resultando em uma especificidade (ou seja, a porcentagem de eventos negativos classificados corretamente) de pouco mais de 99,98%. De fato, essa porcentagem elevada explica por que o modelo teve uma acurácia tão próxima de 100%.\nQuanto ao evento positivo, 261 das 393 transações fraudulentas foram efetivamente classificadas como fraude pelo nosso modelo, resultando em uma sensibilidade de 66,41%. Este valor, de fato, é baixo em comparação com a porcentagem de identificação correta de eventos negativos - e isso já era esperado devido ao desbalanceamento dos dados quanto aos valores da variável resposta; contudo, quando nosso modelo classifica uma transação como fraudulenta (o que ocorreu em 294 transações), ele acertou 261 vezes, resultando em uma precisão de quase 88,78%, um valor significativo.\nEm algumas situações, vale mais a pena garantir que o modelo faça uma previsão certeira dado que ele classificou uma transação como fraudulenta devido à raridade do evento positivo neste tipo de dados. Sendo assim, aceita-se “sacrificar” um pouco da sensibilidade do modelo, aumentando consequentemente a porcentagem de falso-positivos, em nome de uma precisão melhor. Além disto, deve-se avaliar se é mais “custoso” classificar uma transação não-fraudulenta como fraude ou classificar uma transação fraudulenta como não-fraude; nestas condições, analisar o valor (amount) das transações nos respectivos grupos falso-positivos e falso-negativos pode ajudar na tomada de decisão.\nA rechamada (ou recall) é equivalente à sensibilidade e, por se referir ao evento positivo, é tipicamente avaliada em conjunto com a precisão. Por sua vez, a estatística F1-score une a precisão e a rechamada em um único valor que mede a qualidade geral do modelo em relação à classificação do evento positivo. Neste caso, temos que: \\[\nF_1 = \\frac{2 \\cdot precisão \\cdot recall}{precisão + recall} = 0,7598 = 75,98\\%,\n\\] um valor relativamente satisfatório para um grupo de transações que representa somente 0,172% dos dados.\nFinalmente, o coeficiente Kappa (também conhecido como Kappa de Cohen) pode ser visto como uma estatística geral da acurácia de um modelo de classificação porque ele mede o nível de concordância entre a distribuição original dos valores da variável resposta e as previsões geradas pelo modelo. Kappa pode assumir valores entre 0 e 1 e quanto maior o seu valor, melhor é a capacidade preditiva geral do modelo.\nPara os dados de treinamento, o valor de Kappa é um pouco menor que 0,76, o que indica, de acordo com a escala apresentada por (Landis & Koch, 1977), uma concordância “substancial” entre as previsões do modelo e os valores observados.\nPartição de Teste (Modelo Logístico)\nAgora, vamos calcular as previsões do modelo logístico para os dados da partição de teste test_data e que serão armazenadas no vetor previsoes_logit_test. Da mesma forma que foi feito anteriormente, também criaremos um objeto cm_test que conterá todas as informações que precisamos para construir nossa matriz de confusão, assim como gerar as estatísticas de qualidade do ajuste do modelo.\n\nprevisoes_logit_test &lt;- ifelse(predict(modelo_trn,\n                                  newdata = test_data,\n                                  type = \"response\") &gt; 0.5, 1, 0)\n\n\ncm_test &lt;- caret::confusionMatrix(data = as.factor(previsoes_logit_test),\n                              reference = as.factor(test_data$class),\n                              dnn = c(\"Previsto\", \"Observado\"),\n                              positive = \"1\",\n                              mode = \"everything\")\n\nA Figura 4 exibe a matriz de confusão e as estatísticas para o modelo logísico avaliado na partição de teste.\n\ndraw_confusion_matrix(cm_test)\n\n\n\n\n\nFigura 4: Matriz de confusão e estatísticas da qualidade das previsões do modelo logístico na partição de teste\n\n\n\n\n\n\n\nDe uma forma geral, todas as estatísticas apresentaram valores um pouco menores quando comparados aos respectivos valores para o modelo aplicado na partição de treinamento; esta situação não é incomum quando aplicamos um modelo preditivo em dados de teste que não foram utilizado na construção do mesmo, mas a boa notícia é que nosso modelo logístico ainda consegue fazer um bom trabalho em termos de precisão, uma vez que 59 das 72 transações classificadas como fraude eram fraudulentas, resultando em uma precisão de 81,94%, um valor relativamente alto se levarmos em conta o quão desbalanceados são os dados em relação à variável resposta.\nAinda que a sensibilidade (rechamada) em termos de classificação do evento positivo (fraude) tenha caído para cerca de 59,6% na partição de teste, o coeficiente Kappa ainda se manteve próximo de 0,7, indicando que o modelo possui uma capacidade preditiva significativa - o que não nos impede de tentar melhorá-lo, como veremos a seguir."
  },
  {
    "objectID": "posts/fraud-model-kaggle/index.html#modelo-logístico-com-os-preditores-significantes",
    "href": "posts/fraud-model-kaggle/index.html#modelo-logístico-com-os-preditores-significantes",
    "title": "Modelos de Classificação de Fraude para Dados Desbalanceados",
    "section": "\n4.3 Modelo Logístico com os Preditores Significantes",
    "text": "4.3 Modelo Logístico com os Preditores Significantes\nAgora, vamos verificar se manter apenas os oito preditores que foram detectados como estatisticamente significantes no modelo logístico original melhorará as previsões.\nAs configurações do novo modelo construído a partir dos dados da partição de treinamento são armazenadas no objeto modelo_trn_2:\n\nmodelo_trn_2 &lt;- glm(class ~ V4 + V8 + V10 + V14 + V20 + V21 + V22 + V27,\n                    family = binomial(link = \"logit\"),\n                    data = train_data)\n\nAqui, não repetiremos os comandos utilizados na construção das tabelas e figuras, nos concentrando somente na interpretação dos resultados.\nO resumo do novo modelo logístico mostra que, de fato, todos os coeficientes dos preditores continuam sendo estatisticamente significantes:\n\nsummary(modelo_trn_2)\n\n\nCall:\nglm(formula = class ~ V4 + V8 + V10 + V14 + V20 + V21 + V22 + \n    V27, family = binomial(link = \"logit\"), data = train_data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -8.68962    0.14042 -61.881  &lt; 2e-16 ***\nV4           0.79849    0.06411  12.455  &lt; 2e-16 ***\nV8          -0.20602    0.02451  -8.404  &lt; 2e-16 ***\nV10         -0.78856    0.06215 -12.689  &lt; 2e-16 ***\nV14         -0.68713    0.03961 -17.348  &lt; 2e-16 ***\nV20         -0.10124    0.02594  -3.903 9.49e-05 ***\nV21          0.33357    0.03476   9.595  &lt; 2e-16 ***\nV22          0.57372    0.08196   7.000 2.56e-12 ***\nV27         -0.20068    0.03269  -6.138 8.34e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5786.3  on 227845  degrees of freedom\nResidual deviance: 1727.1  on 227837  degrees of freedom\nAIC: 1745.1\n\nNumber of Fisher Scoring iterations: 11\n\n\nA Tabela 7 exibe as medidas de pseudo R-quadrado para o novo modelo, indicando uma pequena queda em relação aos valores do modelo completo com todos os preditores:\n\n\n\nTabela 7: Medidas de pseudo R-quadrado para o novo modelo logístico\n\n\n\n\nMedida\nValor\n\n\n\nMcFadden\n0.70152\n\n\nCoxSnell\n0.01766\n\n\nNagelkerke\n0.70417\n\n\n\n\n\n\n\n\nA Figura 5 mostra que, de uma forma geral, o novo modelo logístico teve um desempenho um pouco pior nos dados de treinamento quanto às estatísticas de qualidade do ajuste. Da mesma forma, a Figura 6 ratifica esta tendência do novo modelo aplicado aos dados da partição de teste.\n\n\n\n\nFigura 5: Matriz de confusão e estatísticas da qualidade das previsões do novo modelo logístico na partição de treinamento\n\n\n\n\n\n\n\n\n\n\n\nFigura 6: Matriz de confusão e estatísticas da qualidade das previsões do novo modelo logístico na partição de teste\n\n\n\n\n\n\n\nCom isto, podemos concluir que a exclusão de preditores não melhorou o modelo logístico original e, portanto, deveríamos manter todos os preditores em um primeiro momento.\nAinda assim, vale observar que, ao contrário da regressão linear clássica, a inclusão de variáveis independentes em um modelo de regressão logística não é garantia de que o modelo melhore - no mínimo, em relação ao valor de uma das medidas de pseudo R-quadrado."
  },
  {
    "objectID": "posts/fraud-model-kaggle/index.html#construção-do-modelo-rf",
    "href": "posts/fraud-model-kaggle/index.html#construção-do-modelo-rf",
    "title": "Modelos de Classificação de Fraude para Dados Desbalanceados",
    "section": "\n5.1 Construção do Modelo RF",
    "text": "5.1 Construção do Modelo RF\nPara a construção do modelo de floresta aleatória (que, por simplificação, chamaremos simplesmente de “modelo RF” deste ponto em diante), vamos transformar os valores da variável resposta class em fatores nas duas partições - este procedimento é obrigatório para que o pacote randomForest construa um modelo de classificação em vez de um modelo de regressão (caso os valores ainda estivessem definidos como numéricos).\n\ntrain_data$class &lt;- factor(train_data$class)\ntest_data$class &lt;- factor(test_data$class)\n\nAgora, vamos construir nosso modelo RF a partir dos dados da partição de treinamento, utilizando a função randomForest. Para isso, solicitaremos que 200 árvores de decisão sejam construídas (ntree = 200) e cada previsão do modelo RF será dada a partir da combinação das previsões dessas árvores. Nós poderíamos ter solicitado um número maior de árvores (ntree = 500, por exemplo), mas o consumo de memória computacional aumenta significativamente nessas condições e, particularmente para os nossos dados, não houve um ganho na qualidade geral do ajuste que justifique utilizar mais do que 200 árvores de decisão.\n\nrf_model &lt;- randomForest::randomForest(class ~ .,\n                                       data = train_data,\n                                       ntree = 200)"
  },
  {
    "objectID": "posts/fraud-model-kaggle/index.html#importância-dos-preditores-no-modelo-rf",
    "href": "posts/fraud-model-kaggle/index.html#importância-dos-preditores-no-modelo-rf",
    "title": "Modelos de Classificação de Fraude para Dados Desbalanceados",
    "section": "\n5.2 Importância dos Preditores no Modelo RF",
    "text": "5.2 Importância dos Preditores no Modelo RF\nDiferente da regressão logística, não é possível verificar, de forma direta, se uma variável preditora é estatisticamente significante para o modelo de floresta aleatória, uma vez que tal modelo não possui parâmetros a serem estimados. Neste caso, adota-se uma abordagem diferente: medir a importância de cada preditor a partir do valor médio da variação da impureza devido a este preditor, onde a média é tomada sob todas as árvores de decisão que fazem parte da floresta. Por padrão, a função randomForest utiliza o coeficiente de Gini como medida de impureza.\nCom o modelo gerado e armazenado no objeto rf_model, é possível acessar a medida de importância calculada a partir dos dados da partição de treinamento. Para simplificar a visualização dos resultados na Tabela 8, a importância é normalizada para que a soma, sob todos os preditores, seja igual a 1. Além disto, nós selecionamos apenas os 10 preditores com os maiores valores, em ordem decrescente de importância.\n\nrf_importance &lt;- as.data.frame(rf_model$importance) |&gt;\n  tibble::rownames_to_column(var = \"Componente\") |&gt;\n  dplyr::mutate(MeanDecreaseGini = MeanDecreaseGini/sum(MeanDecreaseGini) * 100) |&gt;\n  dplyr::arrange(desc(MeanDecreaseGini)) |&gt;\n  dplyr::top_n(n = 10)\n\n\n\n\nTabela 8: Maiores valores de importância das variáveis preditoras no modelo RF\n\n\n\n\nVariável Preditora\nImportância Normalizada (%)\n\n\n\nV17\n19.97\n\n\nV12\n13.35\n\n\nV14\n10.86\n\n\nV11\n8.38\n\n\nV10\n7.86\n\n\nV16\n5.35\n\n\nV9\n3.49\n\n\nV4\n2.97\n\n\nV18\n2.89\n\n\nV7\n2.60\n\n\n\n\n\n\n\n\nAs três variáveis mais importantes para o modelo RF foram, respectivamente, os preditores V17, V12 e V14, responsáveis juntos por quase 44,2% da importância normalizada total - nenhuma das outras variáveis apresentou uma importância normalizada maior que 10%. Curiosamente, dessas três variáveis com as maiores importâncias, somente a última foi detectada como estatisticamente significante no nosso modelo de regressão logística.\nPara mais informações gerais sobre os métodos de cálculo da importância dos preditores, você pode acessar, por exemplo, o tópico da Wikipedia sobre floresta aleatória (em inglês) neste link: https://en.wikipedia.org/wiki/Random_forest."
  },
  {
    "objectID": "posts/fraud-model-kaggle/index.html#avaliação-das-previsões-do-modelo-rf",
    "href": "posts/fraud-model-kaggle/index.html#avaliação-das-previsões-do-modelo-rf",
    "title": "Modelos de Classificação de Fraude para Dados Desbalanceados",
    "section": "\n5.3 Avaliação das Previsões do Modelo RF",
    "text": "5.3 Avaliação das Previsões do Modelo RF\nFinalmente, vamos verificar como o modelo de floresta aleatória se saiu em relação às previsões de fraude, tanto na partição de treinamento quanto na partição de teste. Em ambas as situações, nós criamos um objeto confusionMatrix a partir das previsões (rf_model$predicted e rf_prev_test, respectivamente) e dos valores observados (train_data$class e test_data$class, respectivamente); então, aplicamos a função draw_confusion_matrix para obtermos as respectivas matrizes de confusão e estatísticas da qualidade do ajuste para as duas partições.\nOs resultados são exibidos, respectivamente, na Figura 7 (para a partição de treinamento) e na Figura 8 (para a partição de teste).\n\n5.3.1 Partição de Treinamento (Modelo RF)\n\ncm_rf_trn &lt;- caret::confusionMatrix(data = rf_model$predicted,\n                                    reference = train_data$class,\n                                    positive = \"1\",\n                                    mode = \"everything\")\n\ndraw_confusion_matrix(cm_rf_trn)\n\n\n\nFigura 7: Matriz de confusão e estatísticas da qualidade das previsões do modelo RF na partição de treinamento\n\n\n\n\n\n\n\nTodas as estatísticas de ajuste do modelo RF aplicado aos dados de treinamento apresentaram valores maiores em comparação com o modelo logístico original. Em particular, a precisão do modelo chegou a quase 95,1%, uma vez que 310 das 326 transações classificadas como fraude pela floresta aleatória são, de fato, fraudulentas. A rechamada (sensibilidade) de quase 78,9% para o modelo RF também foi maior em relação ao modelo logístico; como consequência, o F1-score chegou a pouco mais de 86,2%, mostrando que a floresta aleatória tem um poder preditivo maior quanto ao evento positivo (fraude).\nParticularmente, o coeficiente Kappa para os dados de treinamento é um pouco maior que 0,862, o que o indica como “quase perfeito” na escala de concordância (Landis & Koch, 1977). Desta forma, podemos dizer com alguma tranquilidade que o modelo de floresta aleatória teve um desempenho geral superior em comparação ao modelo de regressão logística quando aplicado aos dados da partição de treinamento.\n\n5.3.2 Partição de Teste (Modelo RF)\n\nrf_prev_test &lt;- predict(rf_model, newdata = test_data, type = \"response\")\n\ncm_rf_test &lt;- caret::confusionMatrix(data = rf_prev_test,\n                                     reference = test_data$class,\n                                     positive = \"1\",\n                                     mode = \"everything\")\n\ndraw_confusion_matrix(cm_rf_test)\n\n\n\nFigura 8: Matriz de confusão e estatísticas da qualidade das previsões do modelo RF na partição de teste\n\n\n\n\n\n\n\nPara a partição de teste, os resultados do modelo RF também são superiores em relação ao modelo logístico. Particularmente, a precisão (95%), a rechamada (76,8%) e o F1-score (84,9%) indicam que o modelo faz um bom trabalho em prever o evento positivo (fraude).\nNaturalmente, tanto a sensibilidade quanto a especificidade diminuíram, assim como a acurácia, em relação ao modelo RF aplicado aos dados de treinamento, mas essa redução é muito menor em relação ao que foi observado no modelo de regressão logística, ratificando a capacidade da floresta aleatória em minimizar o efeito de superajuste."
  },
  {
    "objectID": "profile/publications.html",
    "href": "profile/publications.html",
    "title": "Publicações",
    "section": "",
    "text": "Cargo: Analista de Dados/Consultor de Negócios Pleno\n\n\n\nElaboração e aplicação de treinamentos presenciais e no formato EAD para a utilização dos softwares estatísticos e de Data Mining da família IBM SPSS (em especial, Modeler e Statistics).\nConsultoria técnica/estatística.\nDesenvolvimento e manutenção de aplicações baseadas em Python e R e que estão integradas com softwares da família IBM SPSS.\n\n\n\n\n\nITI (São Paulo/Brasília, 2024): treinamento adicional de utilização do IBM SPSS Statistics no contexto de automatização das etapas de importação, preparação e análise de dados de certificados digitais; revisão e atualização de um modelo de identificação de fraudes em certificados digitais.\nHonda (São Paulo, 2020): treinamento de modelos de séries temporais no IBM SPSS Statistics para a previsão de vendas de alguns dos principais modelos de moto; realização de análise fatorial com base na opinião de clientes selecionados.\nNissin Foods do Brasil (São Paulo, 2019): treinamento de utilização do IBM SPSS Statistics no contexto de pesquisa de mercado; criação de uma aplicação baseada em R para análise de sensibilidade de preço (PSM).\nCEEE (São Paulo/Porto Alegre, 2018): treinamento de utilização do IBM SPSS Statistics no contexto de análise de séries temporais; treinamento e validação de modelos preditivos de séries temporais para dados de consumo de energia.\n\n\n\n\n\nAnálise estatística: importação e preparação de dados; análise exploratória de dados; análise inferencial; modelagem preditiva; modelos de aprendizado de máquina (machine learning); avaliação e atualização de modelos; construção de relatórios.\nMineração de dados via metodologia CRISP-DM (Cross-Industry Standard Process for Data Mining).\nModelos de classificação: Modelos Lineares Generalizados (Regressão Logística, Modelo Log-Linear de Poisson e Modelo Binomial Negativo); Árvores de Decisão (CHAID, CRT e QUEST); Florestas Aleatórias; Redes Neurais MLP; Support Vector Machines (SVM); Redes Bayesianas; Análise Discriminante.\nModelos de regressão: Modelos Lineares Univariados e Multivariados; Modelos Lineares Mistos; Modelos Lineares Generalizados (Gama e Normal Inverso); Regressão de Cox para dados de sobrevivência.\nModelos de agrupamento: K-Médias; Cluster Hierárquico; TwoStep Cluster.\nAnálise e modelagem de séries temporais via modelos de suavização exponencial e ARIMA.\nLinguagens de programação: Python, R, sintaxe do IBM SPSS Statistics e linguagem CLEM do IBM SPSS Modeler.\nVisualização de dados: IBM SPSS Statistics, IBM SPSS Modeler, Python, R e Microsoft Power BI."
  },
  {
    "objectID": "profile/publications.html#presente-dmss-software-ltda",
    "href": "profile/publications.html#presente-dmss-software-ltda",
    "title": "Publicações",
    "section": "",
    "text": "Cargo: Analista de Dados/Consultor de Negócios Pleno\n\n\n\nElaboração e aplicação de treinamentos presenciais e no formato EAD para a utilização dos softwares estatísticos e de Data Mining da família IBM SPSS (em especial, Modeler e Statistics).\nConsultoria técnica/estatística.\nDesenvolvimento e manutenção de aplicações baseadas em Python e R e que estão integradas com softwares da família IBM SPSS.\n\n\n\n\n\nITI (São Paulo/Brasília, 2024): treinamento adicional de utilização do IBM SPSS Statistics no contexto de automatização das etapas de importação, preparação e análise de dados de certificados digitais; revisão e atualização de um modelo de identificação de fraudes em certificados digitais.\nHonda (São Paulo, 2020): treinamento de modelos de séries temporais no IBM SPSS Statistics para a previsão de vendas de alguns dos principais modelos de moto; realização de análise fatorial com base na opinião de clientes selecionados.\nNissin Foods do Brasil (São Paulo, 2019): treinamento de utilização do IBM SPSS Statistics no contexto de pesquisa de mercado; criação de uma aplicação baseada em R para análise de sensibilidade de preço (PSM).\nCEEE (São Paulo/Porto Alegre, 2018): treinamento de utilização do IBM SPSS Statistics no contexto de análise de séries temporais; treinamento e validação de modelos preditivos de séries temporais para dados de consumo de energia.\n\n\n\n\n\nAnálise estatística: importação e preparação de dados; análise exploratória de dados; análise inferencial; modelagem preditiva; modelos de aprendizado de máquina (machine learning); avaliação e atualização de modelos; construção de relatórios.\nMineração de dados via metodologia CRISP-DM (Cross-Industry Standard Process for Data Mining).\nModelos de classificação: Modelos Lineares Generalizados (Regressão Logística, Modelo Log-Linear de Poisson e Modelo Binomial Negativo); Árvores de Decisão (CHAID, CRT e QUEST); Florestas Aleatórias; Redes Neurais MLP; Support Vector Machines (SVM); Redes Bayesianas; Análise Discriminante.\nModelos de regressão: Modelos Lineares Univariados e Multivariados; Modelos Lineares Mistos; Modelos Lineares Generalizados (Gama e Normal Inverso); Regressão de Cox para dados de sobrevivência.\nModelos de agrupamento: K-Médias; Cluster Hierárquico; TwoStep Cluster.\nAnálise e modelagem de séries temporais via modelos de suavização exponencial e ARIMA.\nLinguagens de programação: Python, R, sintaxe do IBM SPSS Statistics e linguagem CLEM do IBM SPSS Modeler.\nVisualização de dados: IBM SPSS Statistics, IBM SPSS Modeler, Python, R e Microsoft Power BI."
  },
  {
    "objectID": "profile/experience.html",
    "href": "profile/experience.html",
    "title": "Experiência Profissional",
    "section": "",
    "text": "Cargo: Analista de Dados/Consultor de Negócios Pleno\n\n\n\nElaboração e aplicação de treinamentos presenciais e no formato EAD para a utilização dos softwares estatísticos e de Data Mining da família IBM SPSS (em especial, Modeler e Statistics).\nConsultoria técnica/estatística.\nDesenvolvimento e manutenção de aplicações baseadas em Python e R e que estão integradas com softwares da família IBM SPSS.\n\n\n\n\n\nITI (São Paulo/Brasília, 2024): treinamento adicional de utilização do IBM SPSS Statistics no contexto de automatização das etapas de importação, preparação e análise de dados de certificados digitais; revisão e atualização de um modelo de identificação de fraudes em certificados digitais.\nHonda (São Paulo, 2020): treinamento de modelos de séries temporais no IBM SPSS Statistics para a previsão de vendas de alguns dos principais modelos de moto; realização de análise fatorial com base na opinião de clientes selecionados.\nNissin Foods do Brasil (São Paulo, 2019): treinamento de utilização do IBM SPSS Statistics no contexto de pesquisa de mercado; criação de uma aplicação baseada em R para análise de sensibilidade de preço (PSM).\nCEEE (São Paulo/Porto Alegre, 2018): treinamento de utilização do IBM SPSS Statistics no contexto de análise de séries temporais; treinamento e validação de modelos preditivos de séries temporais para dados de consumo de energia.\n\n\n\n\n\nAnálise estatística: importação e preparação de dados; análise exploratória de dados; análise inferencial; modelagem preditiva; modelos de aprendizado de máquina (machine learning); avaliação e atualização de modelos; construção de relatórios.\nMineração de dados via metodologia CRISP-DM (Cross-Industry Standard Process for Data Mining).\nModelos de classificação: Modelos Lineares Generalizados (Regressão Logística, Modelo Log-Linear de Poisson e Modelo Binomial Negativo); Árvores de Decisão (CHAID, CRT e QUEST); Florestas Aleatórias; Redes Neurais MLP; Support Vector Machines (SVM); Redes Bayesianas; Análise Discriminante.\nModelos de regressão: Modelos Lineares Univariados e Multivariados; Modelos Lineares Mistos; Modelos Lineares Generalizados (Gama e Normal Inverso); Regressão de Cox para dados de sobrevivência.\nModelos de agrupamento: K-Médias; Cluster Hierárquico; TwoStep Cluster.\nAnálise e modelagem de séries temporais via modelos de suavização exponencial e ARIMA.\nLinguagens de programação: Python, R, sintaxe do IBM SPSS Statistics e linguagem CLEM do IBM SPSS Modeler.\nVisualização de dados: IBM SPSS Statistics, IBM SPSS Modeler, Python, R e Microsoft Power BI.",
    "crumbs": [
      "Experiência Profissional"
    ]
  },
  {
    "objectID": "profile/experience.html#presente-dmss-software-ltda",
    "href": "profile/experience.html#presente-dmss-software-ltda",
    "title": "Experiência Profissional",
    "section": "",
    "text": "Cargo: Analista de Dados/Consultor de Negócios Pleno\n\n\n\nElaboração e aplicação de treinamentos presenciais e no formato EAD para a utilização dos softwares estatísticos e de Data Mining da família IBM SPSS (em especial, Modeler e Statistics).\nConsultoria técnica/estatística.\nDesenvolvimento e manutenção de aplicações baseadas em Python e R e que estão integradas com softwares da família IBM SPSS.\n\n\n\n\n\nITI (São Paulo/Brasília, 2024): treinamento adicional de utilização do IBM SPSS Statistics no contexto de automatização das etapas de importação, preparação e análise de dados de certificados digitais; revisão e atualização de um modelo de identificação de fraudes em certificados digitais.\nHonda (São Paulo, 2020): treinamento de modelos de séries temporais no IBM SPSS Statistics para a previsão de vendas de alguns dos principais modelos de moto; realização de análise fatorial com base na opinião de clientes selecionados.\nNissin Foods do Brasil (São Paulo, 2019): treinamento de utilização do IBM SPSS Statistics no contexto de pesquisa de mercado; criação de uma aplicação baseada em R para análise de sensibilidade de preço (PSM).\nCEEE (São Paulo/Porto Alegre, 2018): treinamento de utilização do IBM SPSS Statistics no contexto de análise de séries temporais; treinamento e validação de modelos preditivos de séries temporais para dados de consumo de energia.\n\n\n\n\n\nAnálise estatística: importação e preparação de dados; análise exploratória de dados; análise inferencial; modelagem preditiva; modelos de aprendizado de máquina (machine learning); avaliação e atualização de modelos; construção de relatórios.\nMineração de dados via metodologia CRISP-DM (Cross-Industry Standard Process for Data Mining).\nModelos de classificação: Modelos Lineares Generalizados (Regressão Logística, Modelo Log-Linear de Poisson e Modelo Binomial Negativo); Árvores de Decisão (CHAID, CRT e QUEST); Florestas Aleatórias; Redes Neurais MLP; Support Vector Machines (SVM); Redes Bayesianas; Análise Discriminante.\nModelos de regressão: Modelos Lineares Univariados e Multivariados; Modelos Lineares Mistos; Modelos Lineares Generalizados (Gama e Normal Inverso); Regressão de Cox para dados de sobrevivência.\nModelos de agrupamento: K-Médias; Cluster Hierárquico; TwoStep Cluster.\nAnálise e modelagem de séries temporais via modelos de suavização exponencial e ARIMA.\nLinguagens de programação: Python, R, sintaxe do IBM SPSS Statistics e linguagem CLEM do IBM SPSS Modeler.\nVisualização de dados: IBM SPSS Statistics, IBM SPSS Modeler, Python, R e Microsoft Power BI.",
    "crumbs": [
      "Experiência Profissional"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estéfano Souza",
    "section": "",
    "text": "“Oi, eu sou o Estéfano!”\nEste é o espaço no qual compartilho um pouco da minha jornada até aqui, além de alguns tópicos interessantes sobre Estatística, Ciência de Dados, Machine Learning e outras coisas que (provavelmente) só eu acompanho. Fique à vontade para me seguir nesta jornada!\nSou estatístico formado desde 2006 (mais exatamente, desde janeiro de 2007, quando houve a colação de grau oficial) pelo IME-USP, onde também fiz Mestrado (2007-09) e Doutorado (2009-2013), este último com ênfase em Processos Estocásticos baseados em Sistemas Markovianos de Partículas.\nDurante o Bacharelado em Estatística, tive minha primeira experiência prática com modelos de árvores de decisão, assim como meu primeiro contato com a linguagem R, que já dava seus primeiros passos como uma ferramenta para análise e modelagem de dados. Com o tempo, fui aprendendo outras linguagens para a realização de diversas tarefas relacionadas à Ciência de Dados.\nDesde 2014, trabalho de forma direta com análise de dados e dou consultoria e treinamento concentrados no portfólio da família IBM SPSS. Além das linguagens nativas do SPSS Statistics e do SPSS Modeler, também há a possibilidade de integrar estes dois softwares com códigos escritos em Python e R, permitindo o desenvolvimento e manutenção de aplicações baseadas nessas linguagens."
  },
  {
    "objectID": "profile/certifications.html",
    "href": "profile/certifications.html",
    "title": "Certificações e Experiências Complementares",
    "section": "",
    "text": "Certificações profissionais em IBM SPSS Statistics (C2020-011) e IBM SPSS Modeler (C2090-930).\nCertificação em Microsoft Excel – Nível Avançado.\nCertificação em SQL.\nCertificações em Microsoft Power BI – Power BI Desktop e Power BI Services.\nCertificações em Python para preparação, análise, modelagem e visualização de dados via módulos como pandas, numpy, scikit-learn, matplotlib e seaborn.\nCertificações em R para preparação e análise de dados via tidyverse, modelagem de dados via diversos pacotes e visualização de dados via aplicações em shiny.\nCertificações em diversas ferramentas cloud da IBM: Cloud Pak For Data (CP4D); Watson Studio; Watson Machine Learning; Watson OpenScale; Watson Discovery; watsonx.ai; watsonx.governance.\nConhecimentos de deep learning via torch em Python e R.\nVoluntariado na ONG Matemática em Movimento (São Paulo, 2016 – 2024): professor de sala de aula (2016–2019; 2021–2023); coordenador pedagógico (2018–2019), dando suporte para a consolidação das informações de estudantes por meio de dados estruturados e a elaboração da apostila de matemática própria da ONG; diretor pedagógico (2020); mediador de atividades de cultura e desenvolvimento (2023 – 2024).",
    "crumbs": [
      "Certificações e Experiências Complementares"
    ]
  },
  {
    "objectID": "profile/academics.html",
    "href": "profile/academics.html",
    "title": "Formação Acadêmica",
    "section": "",
    "text": "Tese: “Simulação perfeita e aproximações de alcance finito em sistemas de spins com interações de longo alcance”\nOrientador: Antonio Galves\nÁrea: Processos Estocásticos\nPalavras-chave: Acoplamento, Modelo de Ising, Simulação Perfeita, Sistemas Markovianos de Partículas\nLink para acesso: https://www.teses.usp.br/teses/disponiveis/45/45133/tde-27052013-192003/pt-br.php",
    "crumbs": [
      "Formação Acadêmica"
    ]
  },
  {
    "objectID": "profile/academics.html#doutorado-em-estatística-ime-usp",
    "href": "profile/academics.html#doutorado-em-estatística-ime-usp",
    "title": "Formação Acadêmica",
    "section": "",
    "text": "Tese: “Simulação perfeita e aproximações de alcance finito em sistemas de spins com interações de longo alcance”\nOrientador: Antonio Galves\nÁrea: Processos Estocásticos\nPalavras-chave: Acoplamento, Modelo de Ising, Simulação Perfeita, Sistemas Markovianos de Partículas\nLink para acesso: https://www.teses.usp.br/teses/disponiveis/45/45133/tde-27052013-192003/pt-br.php",
    "crumbs": [
      "Formação Acadêmica"
    ]
  },
  {
    "objectID": "profile/academics.html#mestrado-em-estatística-ime-usp",
    "href": "profile/academics.html#mestrado-em-estatística-ime-usp",
    "title": "Formação Acadêmica",
    "section": "2007 – 2009: Mestrado em Estatística (IME-USP)",
    "text": "2007 – 2009: Mestrado em Estatística (IME-USP)\nDissertação: “O problema de Monge-Kantorovich para duas medidas de probabilidade sobre um conjunto finito”\nOrientador: Antonio Galves\nÁrea: Probabilidade\nPalavras-chave: Acoplamento, Monge-Kantorovich, Problema do Transporte Ótimo, Programação Linear\nLink para acesso: https://www.teses.usp.br/teses/disponiveis/45/45133/tde-04052009-162654/pt-br.php",
    "crumbs": [
      "Formação Acadêmica"
    ]
  },
  {
    "objectID": "profile/academics.html#bacharelado-em-estatística-ime-usp",
    "href": "profile/academics.html#bacharelado-em-estatística-ime-usp",
    "title": "Formação Acadêmica",
    "section": "2003 – 2006: Bacharelado em Estatística (IME-USP)",
    "text": "2003 – 2006: Bacharelado em Estatística (IME-USP)\nTrabalhos de conclusão de curso:\n\nRelatório de análise estatística sobre o projeto “Um novo enfoque para o gerenciamento de projetos de desenvolvimento de software” (RAE – CEA – 06P10; 2006)\nResponsáveis pela análise: Júlia Maria Pavan Soler (orientadora); André Giannecchini; Estéfano Alves de Souza\nTécnicas estatísticas utilizadas: Análise Descritiva; Testes de Hipóteses Paramétricas; Análise de Dados Categorizados; Árvores de Classificação e Regressão (CART)\nÁrea de aplicação: Sociometria\nLink para acesso: https://repositorio.usp.br/item/001608049\nRelatório de análise estatística sobre o projeto “Perfil evolutivo da fluência da fala de falantes do português brasileiro” (RAE – CEA – 06P16; 2006)\nResponsáveis pela análise: Viviana Giampaoli (orientadora); Davi Kobayashi Colombo; Estéfano Alves de Souza\nTécnicas estatísticas utilizadas: Análise Descritiva; Testes de Hipóteses Paramétricas; Regressão Logística; Regressão Não-Linear (Modelos de Quase-Verossimilhança)\nÁrea de aplicação: Linguística\nLink para acesso: https://repositorio.usp.br/item/001580003",
    "crumbs": [
      "Formação Acadêmica"
    ]
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Aqui é o espaço no qual apresento algumas ideias sobre Estatística, análise de dados, machine learning e outros tópicos relacionados.\n\n\n\n\n\n\n\n\n\n\n\n\nModelos de Classificação de Fraude para Dados Desbalanceados\n\n\nNosso objetivo principal é explorar alguns recursos de preparação, análise e modelagem de dados dentro do software R, com base em um exemplo de classificação de fraude a…\n\n\n\nEstéfano Souza\n\n\n18-jan-2024\n\n\n\n\n\n\n\n\nNenhum item correspondente"
  }
]